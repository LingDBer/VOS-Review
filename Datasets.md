## Section 3. Datasets

#### Earlier datasets for VOS evaluation
|Years|Datasets with links|Paper links|
|:-:|---|:-:|
|2007|[Hopkins 155](http://www.vision.jhu.edu/data/hopkins155/)|[link](https://www.cis.jhu.edu/~rvidal/publications/cvpr07-benchmark.pdf)|
|2010|[BMS-26](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)|[link](https://link.springer.com/content/pdf/10.1007/978-3-642-15555-0_21.pdf)|
|2013|[FBMS-59](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)|[link](https://ieeexplore.ieee.org/document/6682905)|
|2012|[SegTrack v1](https://cpl.cc.gatech.edu/projects/SegTrack/)|[link](http://www.bmva.org/bmvc/2010/conference/paper56/paper56.pdf)|
|2013|[SegTrack v2](https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html)|[link](https://ieeexplore.ieee.org/document/6751383)|
|2012|[YouTube-Objects](https://vision.cs.utexas.edu/projects/videoseg/)|[link](https://www.cs.utexas.edu/~grauman/papers/suyog-eccv2014.pdf)|
|2015|[JumpCut](https://www.dropbox.com/s/v0v3pkrhz1vizyt/VideoSeg_dataset.rar?dl=0)|[link](https://dl.acm.org/doi/10.1145/2816795.2818105)|

#### :fire: Popular datasets in deep learning era

|Years|Datasets with links|Paper links|Remarks|
|:-:|---|:-:|---|
|2016|[DAVIS-2016](https://davischallenge.org/davis2016/code.html)|[link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf)|(Single object) Applicable to both Unsupervised and Semi-supervised VOS|
|2017|[DAVIS-2017](https://davischallenge.org/davis2017/code.html)|[link](https://arxiv.org/pdf/1704.00675.pdf)|(Multiple objects) Applicable to Semi-supervised VOS (Two popular subsets for evaluation: validation and test-dev. The latter is more challenging.)|
|2019|[DAVIS-2017-U](https://davischallenge.org/davis2017/code.html)|[link](https://arxiv.org/pdf/1704.00675.pdf)|(Multiple objects) Applicable to Unsupervised VOS methods|
|2018|[YouTube-VOS-2018](https://competitions.codalab.org/competitions/19544#participate-get-data)|[link](https://arxiv.org/pdf/1809.03327.pdf)|(Multiple objects) Applicable to Semi-supervised VOS (Registration is required when downloading YouTube-VOS/VIS data)|
|2019|[YouTube-VOS-2019](https://competitions.codalab.org/competitions/20127#participate-get-data)|[link](https://arxiv.org/pdf/1809.03327.pdf)|(Multiple objects) Applicable to Semi-supervised VOS|
|2019|[YouTube-VIS-2019](https://competitions.codalab.org/competitions/20128#participate-get_data)|[link](https://arxiv.org/pdf/1905.04804.pdf)|(Multiple objects) Applicable to Unsupervised VOS|
|2021|[YouTube-VIS-2021](https://competitions.codalab.org/competitions/28988#participate-get_data)|[link](https://arxiv.org/pdf/1905.04804.pdf)|(Multiple objects) Applicable to Unsupervised VOS|

#### Useful dataset for VOS
|Years|Datasets with links|Paper links|
|:-:|---|:-:|
|2019|[SAIL-VOS](http://sailvos.web.illinois.edu/_site/index.html)|[link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_SAIL-VOS_Semantic_Amodal_Instance_Level_Video_Object_Segmentation_-_A_CVPR_2019_paper.pdf)|

